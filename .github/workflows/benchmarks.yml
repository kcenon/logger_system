name: Benchmarks

on:
  push:
    branches: [ main, phase-* ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save as baseline'
        required: false
        default: 'false'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-13]
        compiler: [clang]
        build_type: [Release]

    steps:
    - name: Checkout logger_system
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison
        path: logger_system

    - name: Checkout common_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/common_system
        path: common_system

    - name: Checkout thread_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/thread_system
        path: thread_system

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake ninja-build clang libbenchmark-dev libgtest-dev libfmt-dev

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest fmt

    - name: Set up compiler
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        cmake -B build -S logger_system \
          -GNinja \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DBUILD_BENCHMARKS=ON \
          -DBUILD_TESTS=OFF \
          -DLOGGER_STANDALONE=ON

    - name: Build logger_system
      run: cmake --build build --config ${{ matrix.build_type }} -j

    - name: Run benchmarks
      run: |
        cd build/benchmarks
        echo "Running benchmarks..."

        # Run all benchmarks and output to JSON
        for bench in logger_*_bench object_pool_bench; do
          if [ -f "$bench" ]; then
            echo "Running $bench..."
            ./$bench --benchmark_format=json --benchmark_out=${bench}_results.json || true
          fi
        done

        # Merge all results into single file
        cd ../..
        python3 -c "
import json
import glob
results = {'benchmarks': []}
for f in glob.glob('build/benchmarks/*_results.json'):
    with open(f) as file:
        data = json.load(file)
        if 'benchmarks' in data:
            results['benchmarks'].extend(data['benchmarks'])
        if 'context' not in results and 'context' in data:
            results['context'] = data['context']
with open('build/benchmark_results_${{ matrix.os }}.json', 'w') as out:
    json.dump(results, out, indent=2)
        "

        echo "✅ Benchmarks completed" >> $GITHUB_STEP_SUMMARY

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: build/benchmark_results_${{ matrix.os }}.json
        retention-days: 90

    - name: Save baseline (if requested)
      if: github.event.inputs.save_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        cd logger_system
        mkdir -p benchmarks/baselines
        cp ../build/benchmark_results_${{ matrix.os }}.json \
           benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json

        echo "✅ Baseline saved: benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json" >> $GITHUB_STEP_SUMMARY

    - name: Compare with baseline
      continue-on-error: true
      run: |
        cd logger_system

        # Find latest baseline
        BASELINE=$(ls -t benchmarks/baselines/baseline_${{ matrix.os }}_*.json 2>/dev/null | head -1)

        if [ -f "$BASELINE" ]; then
          echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Comparing with baseline: $BASELINE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run comparison with 5% threshold
          python3 scripts/compare_benchmarks.py \
            "$BASELINE" \
            ../build/benchmark_results_${{ matrix.os }}.json \
            --threshold 5.0 \
            --report ../build/benchmark_comparison.md \
            --fail-on-regression || echo "::warning::Performance regressions detected"

          # Add report to summary
          if [ -f ../build/benchmark_comparison.md ]; then
            cat ../build/benchmark_comparison.md >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ No baseline found for comparison" >> $GITHUB_STEP_SUMMARY
          echo "To create a baseline, run this workflow with 'save_baseline' input set to 'true'" >> $GITHUB_STEP_SUMMARY
        fi

  report:
    name: Generate Benchmark Report
    needs: benchmark
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate summary
      run: |
        echo "# Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Phase 0: Baseline Measurement" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        for dir in benchmark-results-*; do
          if [ -d "$dir" ]; then
            echo "### $dir" >> $GITHUB_STEP_SUMMARY
            if [ -f "$dir/benchmark_results_"*.json ]; then
              echo "✅ Benchmarks completed" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            else
              echo "❌ No results found" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done

        echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review benchmark results" >> $GITHUB_STEP_SUMMARY
        echo "- Document baseline in docs/BASELINE.md" >> $GITHUB_STEP_SUMMARY
        echo "- Set performance targets for Phase 1" >> $GITHUB_STEP_SUMMARY
