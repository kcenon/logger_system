name: Benchmarks

on:
  push:
    branches: [ main, phase-*, feature/ci-performance-automation ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save as baseline'
        required: false
        default: 'false'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-14]
        # Ubuntu uses GCC 13 for std::format+std::jthread, macOS uses Clang
        compiler: [gcc]
        build_type: [Release]

    steps:
    - name: Checkout logger_system
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison
        path: logger_system

    - name: Checkout common_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/common_system
        path: common_system

    - name: Checkout thread_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/thread_system
        ref: feature/adopt-kcenon-feature-flags
        path: thread_system

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        # Install GCC 13 for std::format and std::jthread support
        # (Clang 17 with libstdc++ doesn't support std::format,
        #  and libc++ doesn't support std::jthread)
        sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
        sudo apt-get update
        sudo apt-get install -y \
          cmake ninja-build gcc-13 g++-13 \
          libbenchmark-dev libgtest-dev \
          python3-pip
        # Set GCC 13 as default
        sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 100
        sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-13 100
        pip3 install matplotlib

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest
        # Use --break-system-packages for PEP 668 compliant Python
        pip3 install --break-system-packages matplotlib || pip3 install matplotlib --user

    - name: Set up compiler (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        echo "CC=gcc-13" >> $GITHUB_ENV
        echo "CXX=g++-13" >> $GITHUB_ENV

    - name: Set up compiler (macOS)
      if: runner.os == 'macOS'
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Set matrix environment variables
      run: |
        echo "MATRIX_OS=${{ matrix.os }}" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        # Note: Using GCC 13+ on Ubuntu for both std::format and std::jthread support
        # Clang 17 with libstdc++ doesn't support std::format,
        # and libc++ doesn't support std::jthread yet

        cmake -B build -S logger_system \
          -GNinja \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DBUILD_BENCHMARKS=ON \
          -DBUILD_TESTS=OFF \
          -DLOGGER_STANDALONE_MODE=ON

    - name: Build logger_system
      run: cmake --build build --config ${{ matrix.build_type }} -j

    - name: Run benchmarks
      run: |
        cd build/bin
        echo "Running benchmarks..."

        # List available benchmark binaries
        echo "Available benchmark binaries:"
        ls -la *benchmark* 2>/dev/null || ls -la *bench* 2>/dev/null || echo "No benchmark binaries found"

        # Run the logger_benchmarks binary (built by CMakeLists.txt)
        if [ -f "logger_benchmarks" ]; then
          echo "Running logger_benchmarks..."
          ./logger_benchmarks \
            --benchmark_format=json \
            --benchmark_out=logger_benchmarks_results.json \
            --benchmark_repetitions=3 \
            --benchmark_report_aggregates_only=true \
            --benchmark_min_time=1.0 \
            || true
        else
          echo "âš ï¸ logger_benchmarks binary not found"
          # Try to find in parent benchmarks directory
          if [ -f "../benchmarks/logger_benchmarks" ]; then
            echo "Found in ../benchmarks/, running..."
            ../benchmarks/logger_benchmarks \
              --benchmark_format=json \
              --benchmark_out=logger_benchmarks_results.json \
              --benchmark_repetitions=3 \
              --benchmark_report_aggregates_only=true \
              --benchmark_min_time=1.0 \
              || true
          fi
        fi

        # Merge all results into single file
        cd ../..
        python3 logger_system/scripts/ci/merge_benchmark_results.py

        echo "âœ… Benchmarks completed" >> $GITHUB_STEP_SUMMARY

    - name: Parse benchmark results
      run: |
        cd logger_system
        mkdir -p benchmarks/results

        # Copy merged results
        cp ../build/benchmark_results_${{ matrix.os }}.json \
           benchmarks/results/latest_${{ matrix.os }}.json

        # Parse benchmarks
        python3 scripts/performance/parse_benchmarks.py \
          benchmarks/results/latest_${{ matrix.os }}.json

    - name: Compare with baseline
      continue-on-error: true
      run: |
        cd logger_system

        # Create baseline.json if it doesn't exist (use latest OS-specific baseline)
        if [ ! -f benchmarks/baselines/baseline.json ]; then
          LATEST_BASELINE=$(ls -t benchmarks/baselines/baseline_${{ matrix.os }}_*.json 2>/dev/null | head -1)
          if [ -f "$LATEST_BASELINE" ]; then
            echo "Creating baseline.json from $LATEST_BASELINE"
            # Parse the latest baseline to create summary
            python3 scripts/performance/parse_benchmarks.py "$LATEST_BASELINE"
            cp benchmarks/results/summary.json benchmarks/baselines/baseline.json
          fi
        fi

        # Run comparison
        python3 scripts/performance/compare_baselines.py || \
          echo "::warning::Performance regression detected"

    - name: Generate performance badges
      if: matrix.os == 'ubuntu-22.04'
      run: |
        cd logger_system
        python3 scripts/performance/generate_badges.py

    - name: Generate performance charts
      if: matrix.os == 'ubuntu-22.04'
      run: |
        cd logger_system
        python3 scripts/performance/generate_charts.py

    - name: Update README
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        cd logger_system
        python3 scripts/performance/update_readme.py

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: |
          logger_system/benchmarks/results/
          logger_system/docs/performance/
        retention-days: 90

    - name: Add comparison report to summary
      if: hashFiles('logger_system/benchmarks/results/comparison_report.md') != ''
      run: |
        echo "## ðŸ“Š Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        cat logger_system/benchmarks/results/comparison_report.md >> $GITHUB_STEP_SUMMARY

    - name: Post PR comment
      if: github.event_name == 'pull_request' && matrix.os == 'ubuntu-22.04'
      continue-on-error: true  # Don't fail workflow if comment fails
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const reportPath = 'logger_system/benchmarks/results/comparison_report.md';

          if (fs.existsSync(reportPath)) {
            const report = fs.readFileSync(reportPath, 'utf8');

            try {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: report
              });
              console.log('Successfully posted PR comment');
            } catch (error) {
              console.log('Failed to post PR comment:', error.message);
              console.log('This is non-fatal - benchmark results are in step summary');
            }
          }

    - name: Commit performance updates
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        repository: logger_system
        commit_message: "chore: update performance metrics [skip ci]"
        file_pattern: |
          README.md
          docs/performance/**
          benchmarks/results/**
        commit_user_name: github-actions[bot]
        commit_user_email: github-actions[bot]@users.noreply.github.com

    - name: Save baseline (if requested)
      if: github.event.inputs.save_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        cd logger_system

        # Save OS-specific baseline
        mkdir -p benchmarks/baselines
        cp ../build/benchmark_results_${{ matrix.os }}.json \
           benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json

        # Update main baseline.json
        if [ -f benchmarks/results/summary.json ]; then
          cp benchmarks/results/summary.json benchmarks/baselines/baseline.json
        fi

        echo "âœ… Baseline saved: benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json" >> $GITHUB_STEP_SUMMARY

    # Historical benchmark tracking with github-action-benchmark
    # Only run on main branch push (not PRs) since it requires gh-pages branch access
    - name: Store benchmark results for tracking
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      continue-on-error: true  # Don't fail workflow if benchmark tracking fails
      working-directory: logger_system
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: Logger System Performance
        tool: 'googlecpp'
        output-file-path: benchmarks/results/latest_${{ matrix.os }}.json
        # Store data in gh-pages branch
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: dev/bench
        # Auto-push on main branch
        auto-push: true
        # Alert on performance regression (10% threshold)
        alert-threshold: '110%'
        # Comment on alert
        comment-on-alert: true
        fail-on-alert: false
        # GitHub token for pushing and commenting
        github-token: ${{ secrets.GITHUB_TOKEN }}

    - name: Add performance tracking link to summary
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ðŸ“ˆ Performance Tracking" >> $GITHUB_STEP_SUMMARY
        echo "View historical performance trends: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/" >> $GITHUB_STEP_SUMMARY

  report:
    name: Generate Benchmark Report
    needs: benchmark
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate summary
      run: |
        echo "# ðŸ“Š Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        for dir in benchmark-results-*; do
          if [ -d "$dir" ]; then
            echo "## $dir" >> $GITHUB_STEP_SUMMARY

            # Check for summary
            if [ -f "$dir/benchmarks/results/summary.json" ]; then
              echo "âœ… Benchmarks completed" >> $GITHUB_STEP_SUMMARY

              # Extract key metrics
              python3 scripts/ci/extract_benchmark_metrics.py "$dir/benchmarks/results/summary.json" >> $GITHUB_STEP_SUMMARY

              echo "" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ No results found" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done

        echo "## ðŸ“ˆ Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review detailed results in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Check performance badges at docs/performance/badges/" >> $GITHUB_STEP_SUMMARY
        echo "- View trend charts at docs/performance/charts/" >> $GITHUB_STEP_SUMMARY
