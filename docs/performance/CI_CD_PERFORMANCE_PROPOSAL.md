# CI/CD ì„±ëŠ¥ ì§€í‘œ ìë™í™” ì œì•ˆì„œ

**ë¬¸ì„œ ë²„ì „:** 1.0
**ì‘ì„±ì¼:** 2025-11-04
**í”„ë¡œì íŠ¸:** logger_system
**ëª©ì :** ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ì¶”ì  ë° README ìë™ ì—…ë°ì´íŠ¸

---

## ğŸ“Š Executive Summary

logger_systemì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ CI/CD íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ , README.md ë° ë¬¸ì„œì— ë™ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

**ê¸°ëŒ€ íš¨ê³¼:**
- âœ… ëª¨ë“  ì»¤ë°‹ì—ì„œ ìë™ ì„±ëŠ¥ ì¸¡ì •
- âœ… ì„±ëŠ¥ íšŒê·€ ì¦‰ì‹œ ê°ì§€ (Â±5% ì„ê³„ê°’)
- âœ… README.mdì— ì‹¤ì‹œê°„ ì„±ëŠ¥ ë°°ì§€ í‘œì‹œ
- âœ… íˆìŠ¤í† ë¦¬ì»¬ íŠ¸ë Œë“œ ì°¨íŠ¸ ìë™ ìƒì„±
- âœ… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì•„í‹°íŒ©íŠ¸ë¡œ ë³´ê´€

---

## ğŸ” í˜„ì¬ ìƒíƒœ ë¶„ì„

### âœ… ì¥ì 
1. **ë²¤ì¹˜ë§ˆí¬ ì¸í”„ë¼ ì™„ë¹„**
   - Google Benchmark ê¸°ë°˜ 4ê°œ ë²¤ì¹˜ë§ˆí¬ íŒŒì¼
   - `logger_throughput_bench.cpp`: ì²˜ë¦¬ëŸ‰ ì¸¡ì •
   - `logger_async_bench.cpp`: ë¹„ë™ê¸° ì„±ëŠ¥
   - `logger_write_bench.cpp`: ì“°ê¸° ì§€ì—°ì‹œê°„
   - `logger_rotation_bench.cpp`: íŒŒì¼ íšŒì „ ì„±ëŠ¥

2. **ë¬¸ì„œí™” êµ¬ì¡° ìš°ìˆ˜**
   - `BASELINE.md`: ê¸°ì¤€ ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
   - `benchmarks/README.md`: ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©ë²• ë¬¸ì„œí™”

3. **GitHub Actions ì›Œí¬í”Œë¡œìš° ì¡´ì¬**
   - `.github/workflows/benchmarks.yml` ì´ë¯¸ êµ¬í˜„

### âŒ ê°œì„  í•„ìš” ì‚¬í•­

1. **ë²¤ì¹˜ë§ˆí¬ ë¹„í™œì„±í™”**
   ```yaml
   # í˜„ì¬ ìƒíƒœ (.github/workflows/benchmarks.yml:68)
   -DBUILD_BENCHMARKS=OFF  # âŒ ë¹„í™œì„±í™”ë¨
   ```
   - ì´ìœ : "API changes" (API ë³€ê²½)
   - ì˜í–¥: ì„±ëŠ¥ ì¸¡ì • ë¶ˆê°€

2. **í•˜ë“œì½”ë”©ëœ ì„±ëŠ¥ ì§€í‘œ**
   ```markdown
   <!-- README.md:150-153 -->
   - **Peak Throughput**: Up to 4.34M messages/second
   - **Multi-threaded Performance**:
     - 4 threads: 1.07M messages/s
   ```
   - ìˆ˜ë™ ì—…ë°ì´íŠ¸ í•„ìš”
   - ìµœì‹ ì„± ë³´ì¥ ë¶ˆê°€

3. **ì„±ëŠ¥ íšŒê·€ ê°ì§€ ë¯¸êµ¬í˜„**
   ```yaml
   # benchmarks.yml:113-115
   - name: Check for performance regression
     run: |
       echo "Performance regression detection: TBD"
   ```

4. **ì‹œê°í™” ë¶€ì¬**
   - ì„±ëŠ¥ íŠ¸ë Œë“œ ì°¨íŠ¸ ì—†ìŒ
   - ë°°ì§€(badge) ì—†ìŒ
   - ë¹„êµ ê·¸ë˜í”„ ì—†ìŒ

---

## ğŸ¯ ì œì•ˆ ì†”ë£¨ì…˜ ì•„í‚¤í…ì²˜

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       GitHub Actions Trigger                          â”‚
â”‚  (Push to main, PR, Manual dispatch, Scheduled daily)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Build & Run Benchmarks                                       â”‚
â”‚  - Build logger_system with -DBUILD_BENCHMARKS=ON                    â”‚
â”‚  - Execute: ./build/benchmarks/logger_benchmarks                     â”‚
â”‚  - Output: benchmark_results.json (Google Benchmark JSON format)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Parse & Extract Metrics                                      â”‚
â”‚  - Parse JSON with Python script                                     â”‚
â”‚  - Extract key metrics:                                               â”‚
â”‚    * Throughput (messages/second)                                    â”‚
â”‚    * Latency (P50, P95, P99)                                         â”‚
â”‚    * Memory usage                                                     â”‚
â”‚  - Generate summary.json                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Compare with Baseline                                        â”‚
â”‚  - Load baseline from benchmarks/baselines/baseline.json             â”‚
â”‚  - Calculate delta percentage                                         â”‚
â”‚  - Check regression threshold (Â±5%)                                   â”‚
â”‚  - Generate comparison report                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Generate Visualizations                                      â”‚
â”‚  - Create performance badges (shields.io format)                     â”‚
â”‚  - Generate trend charts (Chart.js/matplotlib)                       â”‚
â”‚  - Update performance_history.json                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Update Documentation                                         â”‚
â”‚  - Update README.md performance section                              â”‚
â”‚  - Update BASELINE.md if new baseline                                â”‚
â”‚  - Commit changes (bot account)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Upload Artifacts & Report                                    â”‚
â”‚  - Upload benchmark_results.json                                     â”‚
â”‚  - Upload comparison_report.md                                        â”‚
â”‚  - Upload trend_charts.png                                            â”‚
â”‚  - Post PR comment with results                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ì œì•ˆ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
logger_system/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ benchmarks.yml              # ì—…ë°ì´íŠ¸ë¨
â”‚       â””â”€â”€ update-performance-badges.yml  # ì‹ ê·œ
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ baselines/
â”‚   â”‚   â”œâ”€â”€ baseline.json               # ê³µì‹ ê¸°ì¤€ì 
â”‚   â”‚   â””â”€â”€ history/                    # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°
â”‚   â”‚       â”œâ”€â”€ 2025-11-04.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ results/                        # ì‹ ê·œ (ìë™ ìƒì„±)
â”‚       â””â”€â”€ latest.json
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ performance/
â”‚       â”œâ”€â”€ badges/                     # ì‹ ê·œ (ìë™ ìƒì„±)
â”‚       â”‚   â”œâ”€â”€ throughput.svg
â”‚       â”‚   â”œâ”€â”€ latency.svg
â”‚       â”‚   â””â”€â”€ memory.svg
â”‚       â”œâ”€â”€ charts/                     # ì‹ ê·œ (ìë™ ìƒì„±)
â”‚       â”‚   â”œâ”€â”€ throughput_trend.png
â”‚       â”‚   â””â”€â”€ latency_trend.png
â”‚       â””â”€â”€ performance_history.json    # ì‹ ê·œ (ì‹œê³„ì—´ ë°ì´í„°)
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ performance/                    # ì‹ ê·œ
        â”œâ”€â”€ parse_benchmarks.py         # JSON íŒŒì‹±
        â”œâ”€â”€ compare_baselines.py        # íšŒê·€ ê²€ì‚¬
        â”œâ”€â”€ generate_badges.py          # ë°°ì§€ ìƒì„±
        â”œâ”€â”€ generate_charts.py          # ì°¨íŠ¸ ìƒì„±
        â””â”€â”€ update_readme.py            # README ì—…ë°ì´íŠ¸
```

---

## ğŸ› ï¸ êµ¬í˜„ ìƒì„¸ (4ë‹¨ê³„)

### Phase 1: ë²¤ì¹˜ë§ˆí¬ ì¬í™œì„±í™” ë° JSON ì¶œë ¥ ì„¤ì •

#### 1.1 GitHub Actions ì›Œí¬í”Œë¡œìš° ìˆ˜ì •

**íŒŒì¼:** `.github/workflows/benchmarks.yml`

**ë³€ê²½ ì‚¬í•­:**

```yaml
# Before
-DBUILD_BENCHMARKS=OFF

# After
-DBUILD_BENCHMARKS=ON
-DLOGGER_STANDALONE=ON
-DCMAKE_BUILD_TYPE=Release
```

#### 1.2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° JSON ì €ì¥

```yaml
- name: Run Benchmarks
  run: |
    cd build/benchmarks

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° JSON ì¶œë ¥
    ./logger_benchmarks \
      --benchmark_format=json \
      --benchmark_out=benchmark_results.json \
      --benchmark_repetitions=10 \
      --benchmark_report_aggregates_only=true \
      --benchmark_min_time=5.0

    # ê²°ê³¼ ë³µì‚¬
    cp benchmark_results.json ../../benchmarks/results/latest.json

- name: Upload Benchmark Results
  uses: actions/upload-artifact@v4
  with:
    name: benchmark-results-${{ matrix.os }}
    path: benchmarks/results/latest.json
    retention-days: 90
```

#### 1.3 API ë³€ê²½ì— ë”°ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ ìˆ˜ì •

**í•„ìš” ì‘ì—…:** ë²¤ì¹˜ë§ˆí¬ ì½”ë“œê°€ ìƒˆ APIì™€ í˜¸í™˜ë˜ë„ë¡ ì—…ë°ì´íŠ¸

```cpp
// ì˜ˆì‹œ: logger_throughput_bench.cpp
// Before (êµ¬ API)
auto logger = std::make_shared<logger_module::logger>();

// After (ì‹  API)
auto logger = kcenon::logger::logger_builder()
    .use_template("production")
    .add_writer("null", std::make_unique<kcenon::logger::null_writer>())
    .build()
    .value();
```

---

### Phase 2: ê²°ê³¼ íŒŒì‹± ë° ê¸°ì¤€ì„  ë¹„êµ

#### 2.1 ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì‹± ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼:** `scripts/performance/parse_benchmarks.py`

```python
#!/usr/bin/env python3
"""
Parse Google Benchmark JSON output and extract key metrics.
"""
import json
import sys
from pathlib import Path
from typing import Dict, Any


def extract_metrics(benchmark_json: Dict[str, Any]) -> Dict[str, Any]:
    """Extract key performance metrics from benchmark results."""

    metrics = {
        "throughput": {},
        "latency": {},
        "memory": {},
        "timestamp": benchmark_json.get("context", {}).get("date", "")
    }

    for benchmark in benchmark_json.get("benchmarks", []):
        name = benchmark["name"]

        # Throughput benchmarks
        if "Throughput" in name:
            if "items_per_second" in benchmark:
                metrics["throughput"][name] = {
                    "messages_per_second": benchmark["items_per_second"],
                    "cpu_time": benchmark["cpu_time"],
                    "real_time": benchmark["real_time"]
                }

        # Latency benchmarks
        elif "Latency" in name or "WriteLatency" in name:
            metrics["latency"][name] = {
                "cpu_time_ns": benchmark["cpu_time"],
                "real_time_ns": benchmark["real_time"]
            }

        # Memory benchmarks
        elif "Memory" in name:
            metrics["memory"][name] = {
                "bytes_allocated": benchmark.get("bytes_per_second", 0)
            }

    return metrics


def calculate_summary(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate summary statistics."""

    summary = {}

    # Max throughput
    if metrics["throughput"]:
        max_throughput = max(
            m["messages_per_second"]
            for m in metrics["throughput"].values()
        )
        summary["peak_throughput_msg_per_sec"] = round(max_throughput)

    # Average latency
    if metrics["latency"]:
        avg_latency = sum(
            m["cpu_time_ns"]
            for m in metrics["latency"].values()
        ) / len(metrics["latency"])
        summary["avg_latency_ns"] = round(avg_latency, 2)

    return summary


def main():
    if len(sys.argv) < 2:
        print("Usage: parse_benchmarks.py <benchmark_results.json>")
        sys.exit(1)

    input_file = Path(sys.argv[1])
    output_file = Path("benchmarks/results/summary.json")

    with open(input_file) as f:
        benchmark_data = json.load(f)

    metrics = extract_metrics(benchmark_data)
    summary = calculate_summary(metrics)

    result = {
        "summary": summary,
        "detailed_metrics": metrics
    }

    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)

    print(f"âœ… Metrics extracted to {output_file}")
    print(f"ğŸ“Š Summary: {summary}")


if __name__ == "__main__":
    main()
```

#### 2.2 ê¸°ì¤€ì„  ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼:** `scripts/performance/compare_baselines.py`

```python
#!/usr/bin/env python3
"""
Compare current benchmark results with baseline and detect regressions.
"""
import json
import sys
from pathlib import Path
from typing import Dict, Any, Tuple


# Regression threshold: 5%
REGRESSION_THRESHOLD = 0.05


def load_json(file_path: Path) -> Dict[str, Any]:
    """Load JSON file."""
    with open(file_path) as f:
        return json.load(f)


def compare_metrics(
    baseline: Dict[str, Any],
    current: Dict[str, Any]
) -> Tuple[bool, Dict[str, Any]]:
    """
    Compare current metrics with baseline.

    Returns:
        (has_regression, comparison_report)
    """

    report = {
        "regression_detected": False,
        "improvements": [],
        "regressions": [],
        "stable": []
    }

    baseline_summary = baseline.get("summary", {})
    current_summary = current.get("summary", {})

    for metric_name, baseline_value in baseline_summary.items():
        current_value = current_summary.get(metric_name)

        if current_value is None:
            continue

        # Calculate percentage change
        delta = (current_value - baseline_value) / baseline_value
        delta_percent = delta * 100

        comparison = {
            "metric": metric_name,
            "baseline": baseline_value,
            "current": current_value,
            "delta_percent": round(delta_percent, 2)
        }

        # For throughput: higher is better
        if "throughput" in metric_name.lower():
            if delta < -REGRESSION_THRESHOLD:  # Significant decrease
                report["regressions"].append(comparison)
                report["regression_detected"] = True
            elif delta > REGRESSION_THRESHOLD:  # Significant increase
                report["improvements"].append(comparison)
            else:
                report["stable"].append(comparison)

        # For latency: lower is better
        elif "latency" in metric_name.lower():
            if delta > REGRESSION_THRESHOLD:  # Significant increase
                report["regressions"].append(comparison)
                report["regression_detected"] = True
            elif delta < -REGRESSION_THRESHOLD:  # Significant decrease
                report["improvements"].append(comparison)
            else:
                report["stable"].append(comparison)

    return report["regression_detected"], report


def generate_markdown_report(report: Dict[str, Any]) -> str:
    """Generate Markdown report."""

    md = ["# Performance Comparison Report\n"]

    if report["regression_detected"]:
        md.append("## âš ï¸ Performance Regressions Detected\n")
        for item in report["regressions"]:
            md.append(f"- **{item['metric']}**: "
                     f"{item['baseline']} â†’ {item['current']} "
                     f"({item['delta_percent']:+.2f}%)\n")
    else:
        md.append("## âœ… No Performance Regressions\n")

    if report["improvements"]:
        md.append("\n## ğŸš€ Performance Improvements\n")
        for item in report["improvements"]:
            md.append(f"- **{item['metric']}**: "
                     f"{item['baseline']} â†’ {item['current']} "
                     f"({item['delta_percent']:+.2f}%)\n")

    if report["stable"]:
        md.append("\n## âš–ï¸ Stable Metrics\n")
        for item in report["stable"]:
            md.append(f"- **{item['metric']}**: "
                     f"{item['current']} "
                     f"({item['delta_percent']:+.2f}%)\n")

    return "".join(md)


def main():
    baseline_file = Path("benchmarks/baselines/baseline.json")
    current_file = Path("benchmarks/results/summary.json")
    report_file = Path("benchmarks/results/comparison_report.md")

    baseline = load_json(baseline_file)
    current = load_json(current_file)

    has_regression, report = compare_metrics(baseline, current)

    # Save JSON report
    report_json_file = Path("benchmarks/results/comparison_report.json")
    with open(report_json_file, 'w') as f:
        json.dump(report, f, indent=2)

    # Save Markdown report
    md_report = generate_markdown_report(report)
    with open(report_file, 'w') as f:
        f.write(md_report)

    print(md_report)

    # Exit with error code if regression detected
    if has_regression:
        print("\nâŒ Performance regression detected!")
        sys.exit(1)
    else:
        print("\nâœ… No performance regression!")
        sys.exit(0)


if __name__ == "__main__":
    main()
```

---

### Phase 3: ë°°ì§€ ë° ì°¨íŠ¸ ìƒì„±

#### 3.1 ì„±ëŠ¥ ë°°ì§€ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼:** `scripts/performance/generate_badges.py`

```python
#!/usr/bin/env python3
"""
Generate performance badges using shields.io format.
"""
import json
from pathlib import Path
from typing import Dict, Any


def format_throughput(value: float) -> str:
    """Format throughput value."""
    if value >= 1_000_000:
        return f"{value / 1_000_000:.2f}M"
    elif value >= 1_000:
        return f"{value / 1_000:.1f}K"
    else:
        return f"{value:.0f}"


def format_latency(value_ns: float) -> str:
    """Format latency value."""
    if value_ns >= 1_000_000:
        return f"{value_ns / 1_000_000:.2f}ms"
    elif value_ns >= 1_000:
        return f"{value_ns / 1_000:.2f}Î¼s"
    else:
        return f"{value_ns:.0f}ns"


def get_badge_color(metric_name: str, value: float) -> str:
    """Determine badge color based on performance."""

    # Throughput thresholds (messages/sec)
    if "throughput" in metric_name.lower():
        if value >= 4_000_000:
            return "brightgreen"
        elif value >= 2_000_000:
            return "green"
        elif value >= 1_000_000:
            return "yellow"
        else:
            return "orange"

    # Latency thresholds (nanoseconds)
    elif "latency" in metric_name.lower():
        if value <= 500:
            return "brightgreen"
        elif value <= 1000:
            return "green"
        elif value <= 5000:
            return "yellow"
        else:
            return "orange"

    return "blue"


def generate_badge_url(label: str, value: str, color: str) -> str:
    """Generate shields.io badge URL."""
    return (
        f"https://img.shields.io/badge/"
        f"{label.replace(' ', '%20')}-"
        f"{value.replace(' ', '%20')}-"
        f"{color}"
    )


def generate_static_badge_svg(
    label: str,
    value: str,
    color: str,
    output_file: Path
):
    """Generate static SVG badge file."""

    # Simple SVG badge template
    svg_template = f"""<svg xmlns="http://www.w3.org/2000/svg" width="160" height="20">
  <linearGradient id="b" x2="0" y2="100%">
    <stop offset="0" stop-color="#bbb" stop-opacity=".1"/>
    <stop offset="1" stop-opacity=".1"/>
  </linearGradient>
  <mask id="a">
    <rect width="160" height="20" rx="3" fill="#fff"/>
  </mask>
  <g mask="url(#a)">
    <path fill="#555" d="M0 0h90v20H0z"/>
    <path fill="#{get_color_hex(color)}" d="M90 0h70v20H90z"/>
    <path fill="url(#b)" d="M0 0h160v20H0z"/>
  </g>
  <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
    <text x="45" y="15" fill="#010101" fill-opacity=".3">{label}</text>
    <text x="45" y="14">{label}</text>
    <text x="125" y="15" fill="#010101" fill-opacity=".3">{value}</text>
    <text x="125" y="14">{value}</text>
  </g>
</svg>"""

    with open(output_file, 'w') as f:
        f.write(svg_template)


def get_color_hex(color_name: str) -> str:
    """Convert color name to hex."""
    colors = {
        "brightgreen": "4c1",
        "green": "97ca00",
        "yellow": "dfb317",
        "orange": "fe7d37",
        "red": "e05d44",
        "blue": "007ec6"
    }
    return colors.get(color_name, "007ec6")


def main():
    summary_file = Path("benchmarks/results/summary.json")
    badges_dir = Path("docs/performance/badges")
    badges_dir.mkdir(parents=True, exist_ok=True)

    with open(summary_file) as f:
        data = json.load(f)

    summary = data.get("summary", {})

    # Generate badges
    badges_info = []

    # Throughput badge
    if "peak_throughput_msg_per_sec" in summary:
        throughput = summary["peak_throughput_msg_per_sec"]
        value_str = format_throughput(throughput) + " msg/s"
        color = get_badge_color("throughput", throughput)

        generate_static_badge_svg(
            "throughput",
            value_str,
            color,
            badges_dir / "throughput.svg"
        )

        badges_info.append({
            "name": "Throughput",
            "file": "docs/performance/badges/throughput.svg",
            "url": generate_badge_url("throughput", value_str, color)
        })

    # Latency badge
    if "avg_latency_ns" in summary:
        latency = summary["avg_latency_ns"]
        value_str = format_latency(latency)
        color = get_badge_color("latency", latency)

        generate_static_badge_svg(
            "latency",
            value_str,
            color,
            badges_dir / "latency.svg"
        )

        badges_info.append({
            "name": "Latency",
            "file": "docs/performance/badges/latency.svg",
            "url": generate_badge_url("latency", value_str, color)
        })

    # Save badges info
    badges_json = badges_dir / "badges.json"
    with open(badges_json, 'w') as f:
        json.dump(badges_info, f, indent=2)

    print("âœ… Performance badges generated:")
    for badge in badges_info:
        print(f"  - {badge['name']}: {badge['file']}")


if __name__ == "__main__":
    main()
```

#### 3.2 íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼:** `scripts/performance/generate_charts.py`

```python
#!/usr/bin/env python3
"""
Generate performance trend charts using matplotlib.
"""
import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.dates as mdates


def load_performance_history() -> dict:
    """Load performance history from JSON file."""
    history_file = Path("docs/performance/performance_history.json")

    if not history_file.exists():
        return {"history": []}

    with open(history_file) as f:
        return json.load(f)


def append_to_history(summary: dict):
    """Append current summary to performance history."""
    history_file = Path("docs/performance/performance_history.json")

    history = load_performance_history()

    entry = {
        "timestamp": datetime.now().isoformat(),
        "metrics": summary
    }

    history["history"].append(entry)

    # Keep last 90 days
    if len(history["history"]) > 90:
        history["history"] = history["history"][-90:]

    with open(history_file, 'w') as f:
        json.dump(history, f, indent=2)


def plot_throughput_trend(history: dict, output_file: Path):
    """Plot throughput trend over time."""

    timestamps = []
    throughputs = []

    for entry in history["history"]:
        if "peak_throughput_msg_per_sec" in entry["metrics"]:
            timestamps.append(datetime.fromisoformat(entry["timestamp"]))
            throughputs.append(entry["metrics"]["peak_throughput_msg_per_sec"])

    if not timestamps:
        print("âš ï¸ No throughput data to plot")
        return

    plt.figure(figsize=(12, 6))
    plt.plot(timestamps, throughputs, marker='o', linestyle='-', linewidth=2)
    plt.title("Logger Throughput Trend", fontsize=16, fontweight='bold')
    plt.xlabel("Date", fontsize=12)
    plt.ylabel("Messages/Second", fontsize=12)
    plt.grid(True, alpha=0.3)

    # Format y-axis
    ax = plt.gca()
    ax.yaxis.set_major_formatter(
        plt.FuncFormatter(lambda x, p: f'{x/1_000_000:.1f}M')
    )

    # Format x-axis
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.savefig(output_file, dpi=150)
    print(f"âœ… Throughput chart saved to {output_file}")


def plot_latency_trend(history: dict, output_file: Path):
    """Plot latency trend over time."""

    timestamps = []
    latencies = []

    for entry in history["history"]:
        if "avg_latency_ns" in entry["metrics"]:
            timestamps.append(datetime.fromisoformat(entry["timestamp"]))
            latencies.append(entry["metrics"]["avg_latency_ns"])

    if not timestamps:
        print("âš ï¸ No latency data to plot")
        return

    plt.figure(figsize=(12, 6))
    plt.plot(timestamps, latencies, marker='o', linestyle='-',
             linewidth=2, color='orange')
    plt.title("Logger Latency Trend", fontsize=16, fontweight='bold')
    plt.xlabel("Date", fontsize=12)
    plt.ylabel("Latency (nanoseconds)", fontsize=12)
    plt.grid(True, alpha=0.3)

    # Format x-axis
    ax = plt.gca()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.xticks(rotation=45)

    plt.tight_layout()
    plt.savefig(output_file, dpi=150)
    print(f"âœ… Latency chart saved to {output_file}")


def main():
    summary_file = Path("benchmarks/results/summary.json")
    charts_dir = Path("docs/performance/charts")
    charts_dir.mkdir(parents=True, exist_ok=True)

    # Load current summary
    with open(summary_file) as f:
        data = json.load(f)
    summary = data.get("summary", {})

    # Append to history
    append_to_history(summary)

    # Load full history
    history = load_performance_history()

    # Generate charts
    plot_throughput_trend(history, charts_dir / "throughput_trend.png")
    plot_latency_trend(history, charts_dir / "latency_trend.png")


if __name__ == "__main__":
    main()
```

---

### Phase 4: README.md ìë™ ì—…ë°ì´íŠ¸

#### 4.1 README ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼:** `scripts/performance/update_readme.py`

```python
#!/usr/bin/env python3
"""
Update README.md with latest performance metrics.
"""
import json
import re
from pathlib import Path
from typing import Dict, Any


def load_summary() -> Dict[str, Any]:
    """Load performance summary."""
    summary_file = Path("benchmarks/results/summary.json")
    with open(summary_file) as f:
        data = json.load(f)
    return data.get("summary", {})


def format_throughput(value: float) -> str:
    """Format throughput value."""
    if value >= 1_000_000:
        return f"{value / 1_000_000:.2f}M"
    elif value >= 1_000:
        return f"{value / 1_000:.0f}K"
    else:
        return f"{value:.0f}"


def update_readme_performance_section(
    readme_content: str,
    summary: Dict[str, Any]
) -> str:
    """Update performance section in README."""

    # Find performance section
    pattern = r'(### ğŸ“Š Performance Benchmarks.*?)\n\n(.*?)(?=\n###|\Z)'

    def replacement(match):
        header = match.group(1)

        # Build new performance section
        new_section = [header, "\n\n"]
        new_section.append("*Benchmarked on Apple M1 (8-core) @ 3.2GHz, 16GB, macOS Sonoma*\n\n")
        new_section.append("<!-- AUTO-GENERATED: Do not edit manually -->\n")

        if "peak_throughput_msg_per_sec" in summary:
            throughput = summary["peak_throughput_msg_per_sec"]
            new_section.append(
                f"- **Peak Throughput**: {format_throughput(throughput)} messages/second\n"
            )

        if "avg_latency_ns" in summary:
            latency = summary["avg_latency_ns"]
            if latency < 1000:
                latency_str = f"{latency:.0f} nanoseconds"
            else:
                latency_str = f"{latency / 1000:.2f} microseconds"
            new_section.append(f"- **Average Latency**: {latency_str}\n")

        new_section.append("\n![Throughput](docs/performance/badges/throughput.svg) ")
        new_section.append("![Latency](docs/performance/badges/latency.svg)\n")

        new_section.append("\n**Performance Trend:**\n\n")
        new_section.append("![Throughput Trend](docs/performance/charts/throughput_trend.png)\n")

        new_section.append("\n<!-- END AUTO-GENERATED -->\n")

        return "".join(new_section)

    updated = re.sub(pattern, replacement, readme_content, flags=re.DOTALL)

    return updated


def main():
    readme_file = Path("README.md")
    summary = load_summary()

    with open(readme_file) as f:
        readme_content = f.read()

    updated_content = update_readme_performance_section(readme_content, summary)

    with open(readme_file, 'w') as f:
        f.write(updated_content)

    print("âœ… README.md updated with latest performance metrics")


if __name__ == "__main__":
    main()
```

#### 4.2 GitHub Actionsì—ì„œ ìë™ ì»¤ë°‹

**íŒŒì¼:** `.github/workflows/benchmarks.yml` (ì¶”ê°€ ë¶€ë¶„)

```yaml
- name: Update Documentation with Performance Metrics
  if: github.event_name == 'push' && github.ref == 'refs/heads/main'
  run: |
    # Parse benchmarks
    python3 scripts/performance/parse_benchmarks.py benchmarks/results/latest.json

    # Compare with baseline
    python3 scripts/performance/compare_baselines.py || true

    # Generate badges
    python3 scripts/performance/generate_badges.py

    # Generate charts (requires matplotlib)
    pip3 install matplotlib
    python3 scripts/performance/generate_charts.py

    # Update README
    python3 scripts/performance/update_readme.py

- name: Commit Performance Updates
  if: github.event_name == 'push' && github.ref == 'refs/heads/main'
  uses: stefanzweifel/git-auto-commit-action@v5
  with:
    commit_message: "chore: update performance metrics [skip ci]"
    file_pattern: |
      README.md
      docs/performance/**
      benchmarks/results/**
    commit_user_name: github-actions[bot]
    commit_user_email: github-actions[bot]@users.noreply.github.com
```

---

## ğŸ¨ README.md ì„±ëŠ¥ ì„¹ì…˜ ë””ìì¸

### ì œì•ˆ 1: ë°°ì§€ ì¤‘ì‹¬ ë””ìì¸

```markdown
## ğŸ“Š Performance Benchmarks

*Benchmarked on Apple M1 (8-core) @ 3.2GHz, 16GB, macOS Sonoma*

<!-- AUTO-GENERATED: Do not edit manually -->

![Throughput](https://img.shields.io/badge/throughput-4.34M%20msg%2Fs-brightgreen)
![Latency](https://img.shields.io/badge/latency-148ns-brightgreen)
![Memory](https://img.shields.io/badge/memory-2MB-green)

- **Peak Throughput**: 4.34M messages/second (async mode)
- **Average Latency**: 148 nanoseconds (enqueue time)
- **Memory Footprint**: <2MB baseline with adaptive buffer management

**Performance Trend:**

![Throughput Trend](docs/performance/charts/throughput_trend.png)

<!-- END AUTO-GENERATED -->
```

### ì œì•ˆ 2: í…Œì´ë¸” ì¤‘ì‹¬ ë””ìì¸

```markdown
## ğŸ“Š Performance Benchmarks

<!-- AUTO-GENERATED: Do not edit manually -->

| Metric | Value | Status |
|--------|-------|--------|
| **Peak Throughput** | 4.34M msg/s | ![](https://img.shields.io/badge/-brightgreen) |
| **Avg Latency (P50)** | 148 ns | ![](https://img.shields.io/badge/-brightgreen) |
| **Memory Usage** | 2 MB | ![](https://img.shields.io/badge/-green) |
| **Multi-thread (4 threads)** | 1.07M msg/s | ![](https://img.shields.io/badge/-green) |

**Latest Benchmark Run:** 2025-11-04 14:30:00 UTC
**Baseline Comparison:** âœ… No regressions detected

[View Detailed Report](benchmarks/results/comparison_report.md) | [Historical Data](docs/performance/performance_history.json)

<!-- END AUTO-GENERATED -->
```

### ì œì•ˆ 3: ì¹´ë“œ ìŠ¤íƒ€ì¼ ë””ìì¸

```markdown
## ğŸ“Š Performance Benchmarks

<!-- AUTO-GENERATED: Do not edit manually -->

<table>
<tr>
<td align="center">
  <img src="docs/performance/badges/throughput.svg" alt="Throughput" /><br>
  <strong>Peak Throughput</strong><br>
  4.34M msg/s
</td>
<td align="center">
  <img src="docs/performance/badges/latency.svg" alt="Latency" /><br>
  <strong>Avg Latency</strong><br>
  148 ns
</td>
<td align="center">
  <img src="docs/performance/badges/memory.svg" alt="Memory" /><br>
  <strong>Memory Usage</strong><br>
  2 MB
</td>
</tr>
</table>

**Performance Trends** (Last 30 days)

![](docs/performance/charts/throughput_trend.png)

<!-- END AUTO-GENERATED -->
```

---

## ğŸ”„ CI/CD ì›Œí¬í”Œë¡œìš° ì „ì²´ ì˜ˆì‹œ

**íŒŒì¼:** `.github/workflows/benchmarks.yml` (ì™„ì „ ë²„ì „)

```yaml
name: Benchmarks

on:
  push:
    branches: [ main, phase-* ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save as new baseline'
        required: false
        default: 'false'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-13]
        compiler: [clang]
        build_type: [Release]

    steps:
    - name: Checkout logger_system
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        path: logger_system

    - name: Checkout common_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/common_system
        path: common_system

    - name: Checkout thread_system
      uses: actions/checkout@v4
      with:
        repository: kcenon/thread_system
        path: thread_system

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          cmake ninja-build clang \
          libbenchmark-dev libgtest-dev libfmt-dev \
          python3-pip
        pip3 install matplotlib scipy

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest fmt
        pip3 install matplotlib scipy

    - name: Set up compiler
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        cmake -B build -S logger_system \
          -GNinja \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DBUILD_BENCHMARKS=ON \
          -DBUILD_TESTS=OFF \
          -DLOGGER_STANDALONE=ON

    - name: Build logger_system
      run: cmake --build build --config ${{ matrix.build_type }} -j

    - name: Run Benchmarks
      run: |
        cd build/benchmarks

        # Run benchmarks with JSON output
        ./logger_benchmarks \
          --benchmark_format=json \
          --benchmark_out=benchmark_results.json \
          --benchmark_repetitions=10 \
          --benchmark_report_aggregates_only=true \
          --benchmark_min_time=5.0

        # Copy results
        mkdir -p ../../logger_system/benchmarks/results
        cp benchmark_results.json \
           ../../logger_system/benchmarks/results/latest_${{ matrix.os }}.json

    - name: Parse Benchmark Results
      run: |
        cd logger_system
        python3 scripts/performance/parse_benchmarks.py \
          benchmarks/results/latest_${{ matrix.os }}.json

    - name: Compare with Baseline
      continue-on-error: true
      run: |
        cd logger_system
        python3 scripts/performance/compare_baselines.py

    - name: Generate Performance Badges
      if: matrix.os == 'ubuntu-22.04'
      run: |
        cd logger_system
        python3 scripts/performance/generate_badges.py

    - name: Generate Performance Charts
      if: matrix.os == 'ubuntu-22.04'
      run: |
        cd logger_system
        python3 scripts/performance/generate_charts.py

    - name: Update README
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        cd logger_system
        python3 scripts/performance/update_readme.py

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: |
          logger_system/benchmarks/results/
          logger_system/docs/performance/
        retention-days: 90

    - name: Post PR Comment
      if: github.event_name == 'pull_request' && matrix.os == 'ubuntu-22.04'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const reportPath = 'logger_system/benchmarks/results/comparison_report.md';

          if (fs.existsSync(reportPath)) {
            const report = fs.readFileSync(reportPath, 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          }

    - name: Commit Performance Updates
      if: matrix.os == 'ubuntu-22.04' && github.event_name == 'push' && github.ref == 'refs/heads/main'
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        repository: logger_system
        commit_message: "chore: update performance metrics [skip ci]"
        file_pattern: |
          README.md
          docs/performance/**
          benchmarks/results/**
        commit_user_name: github-actions[bot]
        commit_user_email: github-actions[bot]@users.noreply.github.com

    - name: Save as Baseline
      if: github.event.inputs.save_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        cd logger_system
        cp benchmarks/results/summary.json \
           benchmarks/baselines/baseline.json

        # Archive old baseline
        mkdir -p benchmarks/baselines/history
        cp benchmarks/baselines/baseline.json \
           benchmarks/baselines/history/baseline_$(date +%Y%m%d).json

  report:
    name: Generate Benchmark Report
    needs: benchmark
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate summary
      run: |
        echo "# ğŸ“Š Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        for dir in benchmark-results-*; do
          if [ -d "$dir" ]; then
            echo "## $dir" >> $GITHUB_STEP_SUMMARY

            if [ -f "$dir/comparison_report.md" ]; then
              cat "$dir/comparison_report.md" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done
```

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼ë¬¼

### 1. GitHub Actions ì‹¤í–‰ ê²°ê³¼

```
âœ… Build logger_system
âœ… Run Benchmarks
âœ… Parse Benchmark Results
  ğŸ“Š Peak Throughput: 4.34M msg/s
  â±ï¸ Avg Latency: 148 ns
âœ… Compare with Baseline
  âœ… No performance regression detected
âœ… Generate Performance Badges
  - throughput.svg
  - latency.svg
  - memory.svg
âœ… Generate Performance Charts
  - throughput_trend.png
  - latency_trend.png
âœ… Update README.md
âœ… Commit changes
```

### 2. PR ìë™ ì½”ë©˜íŠ¸

```markdown
# Performance Comparison Report

## âœ… No Performance Regressions

## ğŸš€ Performance Improvements

- **peak_throughput_msg_per_sec**: 4,200,000 â†’ 4,340,000 (+3.33%)

## âš–ï¸ Stable Metrics

- **avg_latency_ns**: 150 â†’ 148 (-1.33%)
```

### 3. ì—…ë°ì´íŠ¸ëœ README.md

ìë™ìœ¼ë¡œ ìµœì‹  ì„±ëŠ¥ ì§€í‘œê°€ ë°˜ì˜ëœ README.md:

```markdown
## ğŸ“Š Performance Benchmarks

*Benchmarked on Apple M1 (8-core) @ 3.2GHz, 16GB, macOS Sonoma*

<!-- AUTO-GENERATED: Do not edit manually -->

![Throughput](docs/performance/badges/throughput.svg)
![Latency](docs/performance/badges/latency.svg)

- **Peak Throughput**: 4.34M messages/second
- **Average Latency**: 148 nanoseconds

**Performance Trend:**

![Throughput Trend](docs/performance/charts/throughput_trend.png)

<!-- END AUTO-GENERATED -->
```

---

## âœ… êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ì¸í”„ë¼ êµ¬ì¶• (ì˜ˆìƒ ì†Œìš”: 2ì‹œê°„)
- [ ] `scripts/performance/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] `parse_benchmarks.py` ì‘ì„±
- [ ] `compare_baselines.py` ì‘ì„±
- [ ] Python ì˜ì¡´ì„± ì„¤ì¹˜ (`matplotlib`, `scipy`)
- [ ] ë¡œì»¬ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸

### Phase 2: ë°°ì§€ ë° ì°¨íŠ¸ (ì˜ˆìƒ ì†Œìš”: 1.5ì‹œê°„)
- [ ] `generate_badges.py` ì‘ì„±
- [ ] `generate_charts.py` ì‘ì„±
- [ ] `docs/performance/` ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] ìƒ˜í”Œ ë°°ì§€ ë° ì°¨íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸

### Phase 3: README ìë™ ì—…ë°ì´íŠ¸ (ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„)
- [ ] `update_readme.py` ì‘ì„±
- [ ] README.mdì— ìë™ ìƒì„± ë§ˆì»¤ ì¶”ê°€
- [ ] ë¡œì»¬ì—ì„œ README ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸

### Phase 4: CI/CD í†µí•© (ì˜ˆìƒ ì†Œìš”: 2ì‹œê°„)
- [ ] `.github/workflows/benchmarks.yml` ì—…ë°ì´íŠ¸
- [ ] ë²¤ì¹˜ë§ˆí¬ ë¹Œë“œ í™œì„±í™” (`-DBUILD_BENCHMARKS=ON`)
- [ ] ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€ (íŒŒì‹±, ë¹„êµ, ë°°ì§€, ì°¨íŠ¸)
- [ ] ìë™ ì»¤ë°‹ ì•¡ì…˜ ì„¤ì •
- [ ] PR ì½”ë©˜íŠ¸ ê¸°ëŠ¥ ì¶”ê°€

### Phase 5: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (ì˜ˆìƒ ì†Œìš”: 1.5ì‹œê°„)
- [ ] ë¡œì»¬ì—ì„œ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œë®¬ë ˆì´ì…˜
- [ ] í…ŒìŠ¤íŠ¸ ë¸Œëœì¹˜ì—ì„œ CI ì‹¤í–‰ í™•ì¸
- [ ] ì„±ëŠ¥ íšŒê·€ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
- [ ] ë°°ì§€ ë° ì°¨íŠ¸ ì‹œê°í™” ê²€ì¦

### Phase 6: ë¬¸ì„œí™” (ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„)
- [ ] ì´ ì œì•ˆì„œë¥¼ `docs/performance/CI_CD_PERFORMANCE_PROPOSAL.md`ë¡œ ì €ì¥
- [ ] `benchmarks/README.md` ì—…ë°ì´íŠ¸
- [ ] íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì‘ì„±

**ì´ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 9ì‹œê°„**

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### ë¬¸ì œ 1: ë²¤ì¹˜ë§ˆí¬ ë¹Œë“œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
CMake Error: Could not find benchmark
```

**í•´ê²°ì±…:**
```bash
# Ubuntu
sudo apt-get install libbenchmark-dev

# macOS
brew install google-benchmark

# From source
git clone https://github.com/google/benchmark.git
cd benchmark
cmake -B build -DCMAKE_BUILD_TYPE=Release
sudo cmake --build build --target install
```

### ë¬¸ì œ 2: Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜

**ì¦ìƒ:**
```
ModuleNotFoundError: No module named 'matplotlib'
```

**í•´ê²°ì±…:**
```bash
pip3 install matplotlib scipy
```

### ë¬¸ì œ 3: Git ìë™ ì»¤ë°‹ ê¶Œí•œ ì˜¤ë¥˜

**ì¦ìƒ:**
```
Error: refusing to allow a GitHub App to create or update workflow
```

**í•´ê²°ì±…:**

GitHub Actions ì„¤ì • ë³€ê²½:
```yaml
permissions:
  contents: write  # ì¶”ê°€ í•„ìš”
```

ë˜ëŠ” Personal Access Token ì‚¬ìš©:
```yaml
- uses: stefanzweifel/git-auto-commit-action@v5
  with:
    token: ${{ secrets.GITHUB_TOKEN }}
```

### ë¬¸ì œ 4: ë°°ì§€ê°€ READMEì— í‘œì‹œë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:**
ë°°ì§€ ì´ë¯¸ì§€ê°€ ê¹¨ì§

**í•´ê²°ì±…:**

1. ìƒëŒ€ ê²½ë¡œ í™•ì¸:
   ```markdown
   ![Throughput](docs/performance/badges/throughput.svg)
   ```

2. GitHub Pages í™œì„±í™” í•„ìš” ì‹œ:
   ```markdown
   ![Throughput](https://raw.githubusercontent.com/kcenon/logger_system/main/docs/performance/badges/throughput.svg)
   ```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- [Google Benchmark](https://github.com/google/benchmark) - C++ ë²¤ì¹˜ë§ˆí‚¹ í”„ë ˆì„ì›Œí¬
- [Shields.io](https://shields.io/) - ë°°ì§€ ìƒì„± ì„œë¹„ìŠ¤
- [Matplotlib](https://matplotlib.org/) - Python ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
- [GitHub Actions](https://docs.github.com/en/actions) - CI/CD í”Œë«í¼

### ê´€ë ¨ í”„ë¡œì íŠ¸ ì˜ˆì‹œ
- [spdlog Benchmarks](https://github.com/gabime/spdlog#benchmarks)
- [fmt Performance](https://github.com/fmtlib/fmt#performance)

### ë‚´ë¶€ ë¬¸ì„œ
- `benchmarks/README.md` - ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš© ê°€ì´ë“œ
- `BASELINE.md` - ê¸°ì¤€ ì„±ëŠ¥ ì§€í‘œ
- `docs/LOGGER_SYSTEM_ARCHITECTURE.md` - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

ì´ ì œì•ˆì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ë©´ ë‹¤ìŒì„ ë‹¬ì„±í•©ë‹ˆë‹¤:

1. **ìë™í™”**
   - âœ… ëª¨ë“  ì»¤ë°‹ì—ì„œ ìë™ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
   - âœ… README.md ìë™ ì—…ë°ì´íŠ¸ (ìˆ˜ë™ ê°œì… ë¶ˆí•„ìš”)
   - âœ… ì„±ëŠ¥ ë°°ì§€ ì‹¤ì‹œê°„ ê°±ì‹ 

2. **ê°€ì‹œì„±**
   - âœ… READMEì— ìµœì‹  ì„±ëŠ¥ ì§€í‘œ í•­ìƒ í‘œì‹œ
   - âœ… íŠ¸ë Œë“œ ì°¨íŠ¸ë¡œ ì‹œê°ì  ì´í•´ í–¥ìƒ
   - âœ… PRë§ˆë‹¤ ì„±ëŠ¥ ì˜í–¥ ìë™ ë¦¬í¬íŠ¸

3. **í’ˆì§ˆ ë³´ì¦**
   - âœ… ì„±ëŠ¥ íšŒê·€ ì¦‰ì‹œ ê°ì§€ (Â±5% ì„ê³„ê°’)
   - âœ… 90ì¼ê°„ ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ë³´ê´€
   - âœ… ê¸°ì¤€ì„  ëŒ€ë¹„ ì§€ì†ì  ë¹„êµ

4. **ê°œë°œì ê²½í—˜**
   - âœ… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‰½ê²Œ ì ‘ê·¼
   - âœ… ì„±ëŠ¥ ì˜í–¥ ì‚¬ì „ ì¸ì§€ (PR ë‹¨ê³„)
   - âœ… ë¬¸ì„œ ì‘ì„± ë¶€ë‹´ ê°ì†Œ

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **ìŠ¹ì¸ ë° ë¦¬ë·°**
   - ì´ ì œì•ˆì„œ ê²€í† 
   - êµ¬í˜„ ìš°ì„ ìˆœìœ„ ê²°ì •

2. **í”„ë¡œí† íƒ€ì… êµ¬í˜„**
   - Phase 1-2 ë¨¼ì € êµ¬í˜„ (ìŠ¤í¬ë¦½íŠ¸)
   - ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸

3. **CI/CD í†µí•©**
   - Phase 3-4 êµ¬í˜„
   - í…ŒìŠ¤íŠ¸ ë¸Œëœì¹˜ì—ì„œ ê²€ì¦

4. **í”„ë¡œë•ì…˜ ë°°í¬**
   - main ë¸Œëœì¹˜ì— ë¨¸ì§€
   - ì²« ìë™ ì—…ë°ì´íŠ¸ ëª¨ë‹ˆí„°ë§

5. **ìœ ì§€ë³´ìˆ˜ ë° ê°œì„ **
   - í”¼ë“œë°± ìˆ˜ì§‘
   - ì¶”ê°€ ë©”íŠ¸ë¦­ ê³ ë ¤
   - ì°¨íŠ¸ ë””ìì¸ ê°œì„ 

---

**ë¬¸ì„œ ì‘ì„±ì:** Claude (AI Assistant)
**ê²€í†  í•„ìš”:** í”„ë¡œì íŠ¸ ë©”ì¸í…Œì´ë„ˆ
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-04
